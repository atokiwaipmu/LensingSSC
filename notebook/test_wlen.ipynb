{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbodykit\n",
    "from nbodykit.lab import BigFileCatalog\n",
    "from nbodykit.transform import ConcatenateSources, CartesianToEquatorial\n",
    "from nbodykit.cosmology import Planck15\n",
    "import numpy\n",
    "import bigfile\n",
    "\n",
    "from mpi4py import MPI\n",
    "nbodykit.setup_logging()\n",
    "nbodykit.set_options(dask_chunk_size=1024 * 1024)\n",
    "nbodykit.set_options(global_cache_size=0)\n",
    "\n",
    "from nbodykit.utils import DistributedArray, GatherArray\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sigma(ds, dl, zl):\n",
    "    ddls = 1 - numpy.multiply.outer(1 / ds, dl)\n",
    "    ddls = ddls.clip(0)\n",
    "    w = (100. / 3e5) ** 2 * (1 + zl)* dl\n",
    "    inv_sigma_c = (ddls * w)\n",
    "    return inv_sigma_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wlen(Om, dl, zl, ds, Nzs=1):\n",
    "    \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dl, zl: distance and redshift of lensing objects\n",
    "        \n",
    "        ds: distance source plane bins. if a single scalar, do a delta function bin.\n",
    "        \n",
    "        Nzs : number of objects in each ds bin. len(ds) - 1 items\n",
    "        \n",
    "    \"\"\"\n",
    "    ds = numpy.atleast_1d(ds) # promote to 1d, sum will get rid of it\n",
    "    integrand = 1.5 * Om * Nzs * inv_sigma(ds, dl, zl)\n",
    "    Ntot = numpy.sum(Nzs)\n",
    "    w_lensing = numpy.sum(integrand, axis=0) / Ntot\n",
    "    \n",
    "    return w_lensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_map(ipix, npix, weights, localsize, comm):\n",
    "    \"\"\" Make a map from particles, for quantities like\n",
    "    \n",
    "       W(t) = \\int dx delta(t, x) w\n",
    "       \n",
    "       Parameters\n",
    "       ----------\n",
    "       ipix: array_like\n",
    "     \n",
    "       weights : array_like\n",
    "    \n",
    "       Returns\n",
    "       -------\n",
    "       Wmap, Nmap; distributed maps\n",
    "       \n",
    "       Wmap is the weighted map. Nmap is the number of objects\n",
    "    \"\"\"\n",
    "\n",
    "    ipix, labels = numpy.unique(ipix, return_inverse=True)\n",
    "    N = numpy.bincount(labels)\n",
    "    weights = numpy.bincount(labels, weights)\n",
    "    #print(\"shrink to %d from %d\" % (len(ipix), len(labels)))\n",
    "\n",
    "    del labels\n",
    " \n",
    "    pairs = numpy.empty(len(ipix) + 1, dtype=[('ipix', 'i4'), ('N', 'i4'), ('weights', 'f8') ])\n",
    "    pairs['ipix'][:-1] = ipix\n",
    "    pairs['weights'][:-1] = weights\n",
    "    pairs['N'][:-1] = N\n",
    "\n",
    "    pairs['ipix'][-1] = npix - 1 # trick to make sure the final length is correct.\n",
    "    pairs['weights'][-1] = 0\n",
    "    pairs['N'][-1] = 0\n",
    "\n",
    "    disa = DistributedArray(pairs, comm=comm)\n",
    "    disa.sort('ipix')\n",
    "\n",
    "    w = disa['ipix'].bincount(weights=disa['weights'].local, local=False, shared_edges=False)\n",
    "    N = disa['ipix'].bincount(weights=disa['N'].local, local=False, shared_edges=False)\n",
    "\n",
    "    if npix - w.cshape[0] != 0:\n",
    "        if comm.rank == 0:\n",
    "            print('padding -- this shouldnt have occured ', npix, w.cshape)\n",
    "        # pad with zeros, since the last few bins can be empty.\n",
    "        ipadding = DistributedArray.cempty((npix - w.cshape[0],), dtype='i4', comm=comm)\n",
    "        fpadding = DistributedArray.cempty((npix - w.cshape[0],), dtype='f8', comm=comm)\n",
    "\n",
    "        fpadding.local[:] = 0\n",
    "        ipadding.local[:] = 0\n",
    "\n",
    "        w = DistributedArray.concat(w, fpadding)\n",
    "        N = DistributedArray.concat(N, ipadding)\n",
    "\n",
    "    w = DistributedArray.concat(w, localsize=localsize)\n",
    "    N = DistributedArray.concat(N, localsize=localsize)\n",
    "\n",
    "    return w.local, N.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_range(cat, amin, amax):\n",
    "    \"\"\" Read a portion of the lightcone between two red shift ranges\n",
    "\n",
    "        The lightcone from FastPM is sorted in Aemit and an index is built.\n",
    "        So we make use of that.\n",
    "\n",
    "        CrowCanyon is z > 0; We paste the mirror image to form a full sky.\n",
    "    \"\"\"\n",
    "    edges = cat.attrs['aemitIndex.edges']\n",
    "    offsets = cat.attrs['aemitIndex.offset']\n",
    "    start, end = edges.searchsorted([amin, amax])\n",
    "    if cat.comm.rank == 0:\n",
    "        cat.logger.info(\"Range of index is %d to %d\" %(( start + 1, end + 1)))\n",
    "    start = offsets[start + 1][0]\n",
    "    end = offsets[end + 1][0]\n",
    "    cat =  cat.query_range(start, end)\n",
    "    cat1 = cat.copy()\n",
    "    cat1['Position'] = cat1['Position'] * [1, 1, -1.]\n",
    "    cat3 = ConcatenateSources(cat, cat1)\n",
    "    if cat1.csize > 0:\n",
    "        cat3['RA'], cat3['DEC'] = CartesianToEquatorial(cat3['Position'], frame='galactic')\n",
    "    else:\n",
    "        cat3['RA'] = 0\n",
    "        cat3['DEC'] = 0\n",
    "    return cat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpy as healpix\n",
    "def make_kappa_maps(cat, nside, zs_list, ds_list, localsize, nbar):\n",
    "    \"\"\" Make kappa maps at a list of ds\n",
    "\n",
    "        Return kappa, Nm in shape of (n_ds, localsize), kappabar in shape of (n_ds,)\n",
    "\n",
    "        The maps are distributed in memory, and localsize is the size of\n",
    "        map on this rank.\n",
    "    \"\"\"\n",
    "\n",
    "    dl = (abs(cat['Position'] **2).sum(axis=-1)) ** 0.5\n",
    "    chunks = dl.chunks\n",
    "    ra = cat['RA']\n",
    "    dec = cat['DEC']\n",
    "    zl = (1 / cat['Aemit'] - 1)\n",
    "    \n",
    "    ipix = da.apply_gufunc(lambda ra, dec, nside:\n",
    "                           healpix.ang2pix(nside, numpy.radians(90 - dec), numpy.radians(ra)),\n",
    "                        '(),()->()', ra, dec, nside=nside)\n",
    "\n",
    "    npix = healpix.nside2npix(nside)\n",
    "\n",
    "    ipix = ipix.compute()\n",
    "    dl = dl.persist()\n",
    " \n",
    "    cat.comm.barrier()\n",
    "\n",
    "    if cat.comm.rank == 0:\n",
    "        cat.logger.info(\"ipix and dl are persisted\")\n",
    "\n",
    "    area = (4 * numpy.pi / npix) * dl**2\n",
    "\n",
    "    Om = cat.attrs['OmegaM'][0]\n",
    "    \n",
    "    kappa_list = []\n",
    "    kappabar_list = []\n",
    "    Nm_list = []\n",
    "    for zs, ds in zip(zs_list, ds_list):\n",
    "        LensKernel = da.apply_gufunc(lambda dl, zl, Om, ds: wlen(Om, dl, zl, ds), \n",
    "                                     \"(), ()-> ()\",\n",
    "                                     dl, zl, Om=Om, ds=ds)\n",
    "\n",
    "        weights = (LensKernel / (area * nbar))\n",
    "        weights = weights.compute()\n",
    "\n",
    "        cat.comm.barrier()\n",
    "\n",
    "        if cat.comm.rank == 0:\n",
    "            cat.logger.info(\"source plane %g weights are persisted\" % zs)\n",
    "        Wmap, Nmap = weighted_map(ipix, npix, weights, localsize, cat.comm)\n",
    "\n",
    "        cat.comm.barrier()\n",
    "        if cat.comm.rank == 0:\n",
    "            cat.logger.info(\"source plane %g maps generated\" % zs)\n",
    "\n",
    "        # compute kappa bar\n",
    "        # this is a simple integral, but we do not know dl, dz relation\n",
    "        # so do it with values from a subsample of particles\n",
    "        every = (cat.csize // 100000)\n",
    "        \n",
    "        kappa1 = Wmap\n",
    "        if every == 0: every = 1\n",
    "\n",
    "        # use GatherArray, because it is faster than comm.gather at this scale\n",
    "        # (> 4000 ranks on CrayMPI)\n",
    "        ssdl = GatherArray(dl[::every].compute(), cat.comm)\n",
    "        ssLensKernel = GatherArray(LensKernel[::every].compute(), cat.comm)\n",
    "\n",
    "        if cat.comm.rank == 0:\n",
    "            arg = ssdl.argsort()\n",
    "            ssdl = ssdl[arg]\n",
    "            ssLensKernel = ssLensKernel[arg]\n",
    "            \n",
    "            kappa1bar = numpy.trapz(ssLensKernel, ssdl)\n",
    "        else:\n",
    "            kappa1bar = None\n",
    "        kappa1bar = cat.comm.bcast(kappa1bar)\n",
    "\n",
    "        cat.comm.barrier()\n",
    "        if cat.comm.rank == 0:\n",
    "            cat.logger.info(\"source plane %g bar computed \" % zs)\n",
    "        kappa_list.append(kappa1)\n",
    "        kappabar_list.append(kappa1bar)\n",
    "        Nm_list.append(Nmap)\n",
    "    \"\"\"\n",
    "    # estimate nbar\n",
    "    dlmin = dl.min()\n",
    "    dlmax = dl.max()\n",
    "        \n",
    "    volume = (Nmap > 0).sum() / len(Nmap) * 4  / 3 * numpy.pi * (dlmax**3 - dlmin ** 3)\n",
    "    \"\"\"\n",
    "    # returns number rather than delta, since we do not know fsky here.\n",
    "    #Nmap = Nmap / cat.csize * cat.comm.allreduce((Nmap > 0).sum()) # to overdensity.\n",
    "    return numpy.array(kappa_list), numpy.array(kappabar_list), numpy.array(Nm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/bigfile/__init__.py:358: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  return pyxbigfile.Dataset.__init__(self, file, dtype=dtype, size=size)\n",
      "[ 005587.60 ]   0: 02-01 22:46  CatalogSource   INFO     Extra arguments to FileType: () {'dataset': '1/'}\n",
      "[ 005587.60 ]   0: 02-01 22:46  CatalogSource   INFO     Splitting data redshift bins [2.2 2. ]\n"
     ]
    }
   ],
   "source": [
    "zlmin = 2.0\n",
    "zlmax = 2.2\n",
    "zstep = 0.1\n",
    "zs_list = numpy.arange(zlmin, zlmax, zstep)\n",
    "\n",
    "# no need to be accurate here\n",
    "ds_list = Planck15.comoving_distance(zs_list)\n",
    "\n",
    "basedir = '/lustre/work/akira.tokiwa/globus/fastpm/rfof'\n",
    "tiled = 'rfof_proc4096_nc1024_size625_nsteps60lin_ldr0_rcvtrue_fstnone_pnf2_lnf2_s100_dhf1.0000_tiled0.20_fll_elllim_10000_npix_8192_rfofkdt_8_LCDM_10tiled'\n",
    "path = f'{basedir}/{tiled}/usmesh'\n",
    "testpath = '/lustre/work/akira.tokiwa/SRsample/HR/rfof_proc128_nc256_size625_nsteps60lin_ldr0_rcvnil_fstnone_pnf2_lnf2_s200_dhf1.0000_tiled0.20_fll_elllim_10000_npix_512_rfofkdt_8/usmesh/'\n",
    "dataset = '1/'\n",
    "#'/global/cscratch1/sd/yfeng1/m3127/desi/1536-9201-40eae2464/lightcone/usmesh/'\n",
    "\n",
    "cat = BigFileCatalog(testpath, dataset=dataset)\n",
    "\n",
    "kappa = 0\n",
    "Nm = 0\n",
    "kappabar = 0\n",
    "\n",
    "nside = 8192\n",
    "npix = 12 * nside **2#healpix.nside2npix(nside)\n",
    "localsize = npix * (cat.comm.rank + 1) // cat.comm.size - npix * (cat.comm.rank) // cat.comm.size\n",
    "nbar = (cat.attrs['NC'] ** 3  / cat.attrs['BoxSize'] ** 3 * cat.attrs['ParticleFraction'])[0]\n",
    "\n",
    "Nsteps = int(numpy.round((zlmax - zlmin) / zstep))\n",
    "if Nsteps < 2 : Nsteps = 2\n",
    "z = numpy.linspace(zlmax, zlmin, Nsteps, endpoint=True)\n",
    "\n",
    "if cat.comm.rank == 0:\n",
    "    cat.logger.info(\"Splitting data redshift bins %s\" % str(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = z[:-1]\n",
    "z2 = z[1:]\n",
    "amin = 1 / (1 + z1)\n",
    "amax = 1 / (1 + z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 005746.78 ]   0: 02-01 22:49  CatalogSource   INFO     nbar = 0.0687195, zlmin = 2, zlmax = 2.2 zs = [2.  2.1 2.2]\n",
      "[ 005746.78 ]   0: 02-01 22:49  CatalogSource   INFO     Range of index is 33 to 35\n",
      "/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/bigfile/__init__.py:358: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  return pyxbigfile.Dataset.__init__(self, file, dtype=dtype, size=size)\n",
      "[ 005747.10 ]   0: 02-01 22:49  CatalogSource   INFO     read 0 particles\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "if cat.comm.rank == 0:\n",
    "    cat.logger.info(\"nbar = %g, zlmin = %g, zlmax = %g zs = %s\" % (nbar, z2, z1, zs_list))\n",
    "\n",
    "slice = read_range(cat, 1/(1 + z1), 1 / (1 + z2))\n",
    "\n",
    "if cat.comm.rank == 0:\n",
    "    cat.logger.info(\"read %d particles\" % slice.csize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kappa1, kappa1bar, Nm1  \u001b[38;5;241m=\u001b[39m \u001b[43mmake_kappa_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbar\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 24\u001b[0m, in \u001b[0;36mmake_kappa_maps\u001b[0;34m(cat, nside, zs_list, ds_list, localsize, nbar)\u001b[0m\n\u001b[1;32m     21\u001b[0m npix \u001b[38;5;241m=\u001b[39m healpix\u001b[38;5;241m.\u001b[39mnside2npix(nside)\n\u001b[1;32m     23\u001b[0m ipix \u001b[38;5;241m=\u001b[39m ipix\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m---> 24\u001b[0m dl \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m cat\u001b[38;5;241m.\u001b[39mcomm\u001b[38;5;241m.\u001b[39mbarrier()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cat\u001b[38;5;241m.\u001b[39mcomm\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/base.py:287\u001b[0m, in \u001b[0;36mDaskMethodsMixin.persist\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpersist\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Persist this dask collection into memory\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into a Dask collection with the same\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    dask.persist\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/base.py:900\u001b[0m, in \u001b[0;36mpersist\u001b[0;34m(traverse, optimize_graph, scheduler, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m     keys\u001b[38;5;241m.\u001b[39mextend(a_keys)\n\u001b[1;32m    898\u001b[0m     postpersists\u001b[38;5;241m.\u001b[39mappend((rebuild, a_keys, state))\n\u001b[0;32m--> 900\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(keys, results))\n\u001b[1;32m    902\u001b[0m results2 \u001b[38;5;241m=\u001b[39m [r({k: d[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ks}, \u001b[38;5;241m*\u001b[39ms) \u001b[38;5;28;01mfor\u001b[39;00m r, ks, s \u001b[38;5;129;01min\u001b[39;00m postpersists]\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/local.py:557\u001b[0m, in \u001b[0;36mget_sync\u001b[0;34m(dsk, keys, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A naive synchronous version of get_async\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03mCan be useful for debugging.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# if num_workers present, remove it\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronous_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronous_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaiting\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res_info, failed \u001b[38;5;129;01min\u001b[39;00m \u001b[43mqueue_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/concurrent/futures/_base.py:437\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/local.py:542\u001b[0m, in \u001b[0;36mSynchronousExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m fut \u001b[38;5;241m=\u001b[39m Future()\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     fut\u001b[38;5;241m.\u001b[39mset_result(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     fut\u001b[38;5;241m.\u001b[39mset_exception(e)\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/local.py:238\u001b[0m, in \u001b[0;36mbatch_execute_tasks\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_execute_tasks\u001b[39m(it):\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    Batch computing of multiple tasks with `execute_task`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [execute_task(\u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m it]\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/local.py:238\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_execute_tasks\u001b[39m(it):\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    Batch computing of multiple tasks with `execute_task`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m it]\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/local.py:229\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    227\u001b[0m     failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 229\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpack_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m key, result, failed\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/cytoolz/functoolz.pyx:516\u001b[0m, in \u001b[0;36mcytoolz.functoolz.Compose.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/lustre/work/akira.tokiwa/Projects/LensingSSC/env/lib/python3.8/site-packages/dask/array/core.py:418\u001b[0m, in \u001b[0;36m_concatenate2\u001b[0;34m(arrays, axes)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "kappa1, kappa1bar, Nm1  = make_kappa_maps(slice, nside, zs_list, ds_list, localsize, nbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = kappa + kappa1\n",
    "Nm = Nm + Nm1\n",
    "kappabar = kappabar + kappa1bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
